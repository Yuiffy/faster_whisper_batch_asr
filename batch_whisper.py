import sys
import os
import time
import shutil
import traceback
from faster_whisper import WhisperModel, BatchedInferencePipeline

# ================= â„ï¸ RTX 5080 å–æš–é…ç½® â„ï¸ =================
# æ¨¡å‹ï¼šTurbo (é€Ÿåº¦å¿«ï¼Œç²¾åº¦é«˜ï¼Œé€‚åˆæ‰¹é‡)
MODEL_SIZE = "deepdml/faster-whisper-large-v3-turbo-ct2"

# æ˜¾å­˜å¤Ÿå¤§ï¼ŒBatch Size è®¾ä¸º 16 æˆ– 24 å‡å¯
BATCH_SIZE = 16

# æ”¯æŒçš„è§†é¢‘åç¼€ (å¤§å°å†™å‡å¯)
VIDEO_EXTS = {'.mp4', '.flv', '.mkv', '.avi', '.mov', '.webm', '.ts', '.m4v'}
# ===========================================================

def is_video_file(filename):
    return os.path.splitext(filename)[1].lower() in VIDEO_EXTS

def format_timestamp(seconds):
    if seconds is None: return "00:00:00,000"
    ms = int((seconds % 1) * 1000)
    seconds = int(seconds)
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"

def process_one_video(model, batched_model, video_path, file_idx, total_files):
    filename = os.path.basename(video_path)
    output_dir = os.path.dirname(video_path)
    filename_no_ext = os.path.splitext(filename)[0]
    srt_path = os.path.join(output_dir, filename_no_ext + ".srt")

    # --- æ™ºèƒ½è·³è¿‡é€»è¾‘ ---
    if os.path.exists(srt_path):
        print(f"â­ï¸  [è·³è¿‡] å·²å­˜åœ¨å­—å¹•: {filename}")
        return
    # ------------------

    print(f"\nğŸ¬ [{file_idx}/{total_files}] æ­£åœ¨å¤„ç†: {filename}")
    
    try:
        # VAD å‚æ•°é…ç½®
        vad_params = {
            "min_silence_duration_ms": 2000, 
            "speech_pad_ms": 1500,           
        }

        # 1. å¿«é€Ÿåˆ†ææ—¶é•¿
        print("   ğŸ” åˆ†æè§†é¢‘æ—¶é•¿...", end="", flush=True)
        # è¿™é‡Œä¸ºäº†å¿«ï¼Œbatch_size ç”¨å°ä¸€ç‚¹æ¢æµ‹å³å¯ï¼Œä½†ç”¨ batched_model ä¹Ÿè¡Œ
        _, info = batched_model.transcribe(video_path, batch_size=BATCH_SIZE)
        total_duration = info.duration
        print(f" -> {format_timestamp(total_duration)}")

        # 2. å¼€å§‹è½¬å†™
        start_time = time.time()
        
        segments, _ = batched_model.transcribe(
            video_path, 
            batch_size=BATCH_SIZE,
            language="zh",
            initial_prompt="ä»¥ä¸‹æ˜¯äºŒæ¬¡å…ƒè™šæ‹Ÿä¸»æ’­ç›´æ’­å½•åƒï¼Œä¸»è¦ç”¨ç®€ä½“ä¸­æ–‡ã€‚",
            vad_filter=True,            
            vad_parameters=vad_params   
        )

        # å‡†å¤‡è¿›åº¦æ¡
        term_width = shutil.get_terminal_size().columns
        bar_width = max(20, term_width - 50) 

        with open(srt_path, "w", encoding="utf-8") as f:
            for i, segment in enumerate(segments, start=1):
                current_time = segment.end
                percent = (current_time / total_duration) * 100
                if percent > 100: percent = 100
                
                elapsed = time.time() - start_time
                speed = current_time / elapsed if elapsed > 0 else 0 
                eta = (total_duration - current_time) / speed if speed > 0 else 0
                
                filled_len = int(bar_width * percent / 100)
                bar = 'â–ˆ' * filled_len + '-' * (bar_width - filled_len)
                
                # è¿›åº¦æ¡æ˜¾ç¤º
                sys.stdout.write(f"\r   ğŸš€ {percent:5.1f}% [{bar}] ETA:{int(eta)}s | {speed:.0f}x")
                sys.stdout.flush()

                start_str = format_timestamp(segment.start)
                end_str = format_timestamp(segment.end)
                text = segment.text.strip()
                f.write(f"{i}\n{start_str} --> {end_str}\n{text}\n\n")
                
                if i % 10 == 0: f.flush() 

        total_time = time.time() - start_time
        print(f"\n   âœ… å®Œæˆï¼è€—æ—¶: {total_time:.1f}s")

    except Exception as e:
        print(f"\n   âŒ å¤„ç†å¤±è´¥: {filename}")
        print(f"   é”™è¯¯ä¿¡æ¯: {e}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œä¸ºäº†è®©å¾ªç¯ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶

def main():
    os.system('cls' if os.name == 'nt' else 'clear')
    
    if len(sys.argv) < 2:
        print("âŒ è¯·æŠŠã€æ–‡ä»¶å¤¹ã€‘æ‹–æ‹½åˆ° .bat å›¾æ ‡ä¸Šï¼")
        return

    input_path = sys.argv[1]
    
    # 1. æ‰«ææ–‡ä»¶åˆ—è¡¨
    todo_list = []
    print(f"ğŸ“‚ æ­£åœ¨æ‰«æç›®å½•: {input_path}")
    
    if os.path.isfile(input_path):
        # å¦‚æœæ‹–å…¥çš„æ˜¯å•ä¸ªæ–‡ä»¶
        if is_video_file(input_path):
            todo_list.append(input_path)
    else:
        # å¦‚æœæ‹–å…¥çš„æ˜¯ç›®å½• (é€’å½’æ‰«æ)
        for root, dirs, files in os.walk(input_path):
            for file in files:
                if is_video_file(file):
                    full_path = os.path.join(root, file)
                    todo_list.append(full_path)

    total_files = len(todo_list)
    if total_files == 0:
        print("âš ï¸  è¯¥ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶ã€‚")
        return

    print(f"ğŸ“‹ å…±æ‰¾åˆ° {total_files} ä¸ªè§†é¢‘æ–‡ä»¶ã€‚")
    print("=" * 60)

    # 2. åˆå§‹åŒ–æ¨¡å‹ (åªåŠ è½½ä¸€æ¬¡ï¼Œæå¤§èŠ‚çœæ—¶é—´)
    print(f"â³ æ­£åœ¨é¢„çƒ­ RTX 5080 ({MODEL_SIZE})...")
    try:
        model = WhisperModel(MODEL_SIZE, device="cuda", compute_type="float16")
        batched_model = BatchedInferencePipeline(model=model)
        print("ğŸ”¥ å¼•æ“å·²å°±ç»ªï¼Œå–æš–æ¨¡å¼å¯åŠ¨ï¼")
        print("=" * 60)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # 3. å¾ªç¯å¤„ç†
    start_all = time.time()
    
    for idx, video_path in enumerate(todo_list, start=1):
        process_one_video(model, batched_model, video_path, idx, total_files)
    
    end_all = time.time()
    duration = end_all - start_all
    
    print("\n" + "=" * 60)
    print(f"ğŸ† æ‰€æœ‰ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼")
    print(f"â±ï¸  æ€»è€—æ—¶: {int(duration//3600)}å°æ—¶ {int((duration%3600)//60)}åˆ†")
    print("ğŸ›Œ ç¥ä½ å¥½æ¢¦ï¼")

if __name__ == "__main__":
    main()